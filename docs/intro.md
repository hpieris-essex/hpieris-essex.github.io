---
sidebar_position: 0
slug: /
---

# Module Reflection

I’ve been working in the field of machine learning and AI since 2015. However, most of my learning in this field comes from sources such Udacity, Coursera, and other online resources. Through this module my objective was to re-visit some of the foundational elements of machine learning, strengthen my understanding of those concepts and validate my learnings.

The exploratory data analysis unit provided the opportunity to refresh my knowledge and get the exposure to a new EDA workflow. I appreciated the Housing Prices Exploratory Data analysis Jupyter Notebook Liz Coulter-Smith had put together for the unit (Coulter-Smith, 2023); it provides a repeatable workflow that one can use in any exploratory data analysis project. I also appreciated the strong emphasis on why not to shortcut the EDA process. Due to time pressures around projects, I’ve had instances where we moved forward with shallow EDA, only regret it later.  

Corelation and regression unit provided me with another opportunity to revisit the fundamentals. We visited topics such as Pearson correlation, linear regression and polynomial regression in this unit. I have used linear regression in the early days of my machine learning career. It was enjoyable to revisit these topics. This is another area where I saw an opportunity to create project checklists and workflows based on the learnings. For example, I imagine having a repeatable decision tree where we ask questions like what is the objective (predict vs measure strength/ direction of a relationship), are both variables on an equal standing, is causation a consideration. And based on the answers having a clear prescriptive checklist we can follow from there. I’m excited to use this learning to optimize my project workflows in the future.

Linear regression with Scikit-Learn provided the opportunity to re-vist this popular library and freshen-up my understanding. I have used Scikit-Learn library often in the past. However, recently my focus has been on transformer-based models — a type of deep learning models that utilizes self-attention mechanisms to process sequential data (Vaswani et al., 2017). Coming back to Scikit-Learn and reviewing fundamentals such as Linear Regression helped me fully appreciate the importance of these building blocks in more advanced machine learning models.

During the clustering unit we explored, how the unsupervised learning approach of clustering is used to identify various groups within data. A vast amount of unlabeled data is readily available (Nigam et al., 2000); these could only be used in unsupervised implementations unless someone goes through the effort of labeling. For this reason, unsupervised learning algorithms are an important tool in the machine learning toolbox. This unit provided a foundational understanding of K-means clustering, a popular unsupervised clustering technique. I’m excited to apply these clustering techniques with everyday data such as systems logs and telemetry data to discover patterns.

Clustering with Python unit is when our first group project was due. For the group project, I came up with the business question, produced the first version of the exploratory data analysis Colab notebook, and wrote the introduction and conclusion sections of the report. The other team members iterated on the EDA. I was glad to see another team member introducing the elbow method to validate the number of clusters. We had a last minute communications breakdown. But, we were able to recover quickly and submit the project in time. I think overall we worked well as a team. Some of the communications challenges made me appreciate the cultural and background differences among team members in a distributed setting and the importance of leaving room for the team members to both work jointly and separately to advance the project. 

The introduction to ANNs unit provided an overview of how biological neurons function, and how this understanding is modeled in artificial neural networks through weights, biases and layers (Yang et al., 2014). I enjoyed re-visiting the implementations of a simple perceptron, perceptron with an operator and a multi-layer perceptron. The background in biology provided through this unit is quite insightful and expanded my understanding of the ANNs.

The training a neural network unit covered how ANNs learn through trial and error (Mayo, 2017), the role of back propagation, and the building blocks of ANNs. I had the opportunity to re-visit gradient descent. The knowledge and understanding provided through this unit makes me feel comfortable in my ability to explain how ANNs work at a granular level. I believe this foundational understanding is going to be quite useful as I’m venturing into building more complex models. I also enjoyed writing the forum post on AI writers, a timely topic considering the widespread use of large language models in all aspects of content creation tasks.

The introduction to convolutional neural networks unit covered the algorithm and model architecture for building a CNN. My work in machine learning thus far has been focused on tabular and text data. I enjoyed the opportunity to dive into computer vision through this unit. The video content on concepts such as convolution operation, filters and pooling provided a strong foundational understanding of the building blocks of a CNN. I look forward to exploring more computer vision use cases with my newfound knowledge of convolutional neural networks.

In the CNN interactive learning unit we further explored convolutional neural networks. It was interesting to see the visual modelling of how neural networks work. I have briefly looked at visualization tools like Tensorboard in the past (Ganegedara, 2018). However, I hadn’t fully explored them before. I found it useful to get some re-exposure to various visualization tools. I see them as valuable tools for communicating model architectures and complexities with various stakeholders in the future.

The model selection and evaluation provides a comprehensive overview of approaches for arriving at a high-performing model, and the type of challenges we would encounter in the process. I have working knowledge of model selection and evaluation through my machine learning work in the past. However, reviewing this information was useful to expand my understanding of the topic. I look forward to converting the learning from this unit into a repeatable process that I could apply for model selection and evaluation in my projects in the future.

In this final unit of the machine learning module — Industry 4.0 and Machine Learning, we covered the smartisation of manufacturing industries through the use of machine learning. It was interesting to explore robotics use cases in manufacturing and how machine learning would be instrumental in producing high levels of efficiency and cost reduction in industry 4.0.

Throughout the module I enjoyed the opportunity to re-visit and re-enforce my understanding of machine learning fundamentals. I was wable to identify repeatable frameworks and processes that I could use in my machine learning projects from the provided examples and coursework. I also noticed the emphasis on visiting social and ethical issues associated with machine learning throughout the module. As a result, I’ve come to appreciate the importance of considering the social and ethical issues associated with machine learning initiatives not as an afterthought but rather as a pillar of the initiative itself. 

I look forward to leveraging the learnings from this module to deliver high-quality machine learning implementations more efficiently while being mindful of the social and ethical issues that may arise from my work.


#### References

Coulter-Smith, L. (2023) FIXED-14-11-23_Unit02 A Tutorial on Exploratory Data Analysis.ipynb. Available from: https://colab.research.google.com/drive/1Bwv5f-ciu9ac8Nnf9o98iBvKVsKOJQhN [Accessed 19 February 2024].

Ganegedara, T. (2018) TensorBoard Tutorial. Available from: https://www.datacamp.com/tutorial/tensorboard-tutorial [Accessed 19 February 2024].

Mayo, M. (2017) Neural Network Foundations, Explained: Updating Weights with Gradient Descent & Backpropagation. Available from: https://www.kdnuggets.com/2017/10/neural-network-foundations-explained-gradient-descent.html [Accessed 19 February 2024].

Nigam, K., Mccallum, A., Thrun, S. & Mitchell, T.(2000) Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning. Machine Learning 39: 103–134. DOI: https://doi.org/10.1023/A:1007692713085

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L. & Polosukhin, I. (2017) Attention Is All You Need. Available from: https://doi.org/10.48550/arXiv.1706.03762 [Accessed 19 February 2024].

Yang, Z., Yang Z. & A., Brahme. (2014) 'Introduction', in Yang, Z. & Yang Z. (eds) *Comprehensive Biomedical Physics*. Elsevier. Amsterdam. 1-17.
