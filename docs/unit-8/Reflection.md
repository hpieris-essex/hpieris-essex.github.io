---
sidebar_position: 1
---

The training a neural network unit covered how ANNs learn through trial and error (Mayo, 2017), the role of back propagation, and the building blocks of ANNs. I had the opportunity to re-visit gradient descent. The knowledge and understanding provided through this unit makes me feel comfortable in my ability to explain how ANNs work at a granular level. I believe this foundational understanding is going to be quite useful as Iâ€™m venturing into building more complex models. I also enjoyed writing the forum post on AI writers, a timely topic considering the widespread use of large language models in all aspects of content creation tasks.

References

Mayo, M. (2017) Neural Network Foundations, Explained: Updating Weights with Gradient Descent & Backpropagation. Available from: https://www.kdnuggets.com/2017/10/neural-network-foundations-explained-gradient-descent.html [Accessed 19 February 2024].
