"use strict";(self.webpackChunkessex_portfolio_ml=self.webpackChunkessex_portfolio_ml||[]).push([[9929],{121:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var a=t(4848),n=t(8453);const s={sidebar_position:15},o=void 0,r={id:"rd-docs/Unit 10: Research Proposal",title:"Unit 10: Research Proposal",description:"Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques",source:"@site/docs/rd-docs/Unit 10: Research Proposal.md",sourceDirName:"rd-docs",slug:"/rd-docs/Unit 10: Research Proposal",permalink:"/rd-docs/Unit 10: Research Proposal",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:15,frontMatter:{sidebar_position:15},sidebar:"rdSidebar",previous:{title:"Unit 7: Literature Review",permalink:"/rd-docs/Unit 7: Literature Review"},next:{title:"Unit 8-9: Statistical Worksheet and Charts",permalink:"/rd-docs/Unit 8-9: Statistical Worksheet and Charts"}},l={},c=[{value:"Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques",id:"optimizing-transformer-models-for-sentiment-analysis-a-comparative-study-of-deep-learning-techniques",level:3}];function d(e){const i={a:"a",h3:"h3",p:"p",...(0,n.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.h3,{id:"optimizing-transformer-models-for-sentiment-analysis-a-comparative-study-of-deep-learning-techniques",children:"Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques"}),"\n",(0,a.jsx)("iframe",{src:"https://drive.google.com/file/d/1Zj2paOYdb9YyB5PJuUOEvuHZmJ5XL0dy/preview",width:"640",height:"480",allow:"autoplay"}),"\n",(0,a.jsx)(i.p,{children:'Welcome to my research proposal presentation. The title of the research is "Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques." The goal of my project is to enhance sentiment analysis using transformer models like BERT and Llama.'}),"\n",(0,a.jsx)(i.p,{children:"Llama, similar to BERT, is designed to understand the context of words within a sentence, which is very important for accurate text analysis. However, both these models are computationally intensive. They require significant resources for training and deployment. This research adopts a conclusive research methodology to answer a specific research question and achieve well-defined objectives. Throughout this presentation, I'm going to cover the research problem, methodology, data collection, model optimization techniques, evaluation methods, and ethical considerations. Let's get started."}),"\n",(0,a.jsx)(i.p,{children:"Transformer models such as BERT and Llama have demonstrated great performance in sentiment analysis due to their ability to capture context and nuances in a document or body of text. However, these models, as I mentioned before, are expensive because they require high-end hardware like GPUs or TPUs. This results in high costs for both building and operating these models. Given this cost challenge, there is a critical need for models like these to perform sentiment analysis efficiently and cost-effectively while maintaining high accuracy. My research will explore optimization techniques that could make these transformer models more accessible and feasible for real-world applications without compromising their performance. Through my conclusive research approach, I will test various strategies to balance cost-effectiveness with accuracy."}),"\n",(0,a.jsx)(i.p,{children:"The central research question in my research is: how can transformer-based models be optimized for low computational costs while maintaining accuracy? Addressing this will help make sentiment analysis more accessible by reducing the resource requirements to run powerful models."}),"\n",(0,a.jsx)(i.p,{children:"We have a three-step process to answer this question."}),"\n",(0,a.jsx)(i.p,{children:"The first step is to compare transformer models, specifically BERT and Llama, to evaluate their performance in sentiment analysis. This comparison will help identify each model's strengths and limitations. Second, I will develop a set of resource-efficient transformer models by applying optimization techniques like pruning, quantization, and knowledge distillation to reduce the computational costs. Last, I will measure the cost-performance delta between the original and optimized models."}),"\n",(0,a.jsx)(i.p,{children:"This involves assessing how much we can lower computational requirements without significantly compromising accuracy. Understanding this trade-off will help us recommend the most effective optimization strategies."}),"\n",(0,a.jsx)(i.p,{children:"The key literature that will provide the foundation for this research includes Vaswani et al. (2017), which introduced the transformer architecture. The transformer architecture revolutionized natural language processing by using a mechanism called self-attention to capture relationships between all words in a sentence or body of text. This improves context understanding. This mechanism involves weighting each word's importance relative to the others and is repeated multiple times using attention heads. This design enhances performance, but it is computationally intensive because as the length of the body of text grows, the memory and compute requirements can grow dramatically."}),"\n",(0,a.jsx)(i.p,{children:"Building on this architecture, Devlin et al. (2019) introduced BERT, a Bidirectional Encoder Representations from Transformers. BERT uses a bidirectional approach, considering context from both sides of a word simultaneously, which increases the memory and compute requirements even more. Llama by Touvron et al. (2023), scales these models with parameters ranging from 7 billion to 70 billion. This significantly increases the computational requirements. Despite the performance, these models are costly to operate."}),"\n",(0,a.jsx)(i.p,{children:"This highlights the need for optimization. Cambria (2016) and Zhang et al. (2018) discuss the additional challenges in sentiment analysis, such as imbalance and resource requirements. These challenges emphasize the importance of optimizing models to make them both accessible and efficient."}),"\n",(0,a.jsx)(i.p,{children:"Let's go over the methodology that I plan to use. As I mentioned before, I am using conclusive research. Unlike exploratory research, which is more open-ended, conclusive research is structured and aimed at providing specific, actionable results."}),"\n",(0,a.jsx)(i.p,{children:"Here, I focus on optimizing transformer models to balance performance and computational cost. For data collection, I will use quantitative observation, drawing on publicly available sentiment datasets like IMDb, Twitter, and Amazon sentiment datasets to gather numerical metrics. During this process, I will conduct data quality checks to ensure the datasets are clean, balanced, and free of inconsistencies. For model selection, I plan to implement BERT and Llama, which are both publicly available through sources like Hugging Face."}),"\n",(0,a.jsx)(i.p,{children:"My planned optimization techniques include pruning, which removes less significant neurons to reduce model size and computational costs (Liu et al., 2019); quantization, which involves lowering parameter precision to decrease memory usage (Clark, 2024); and knowledge distillation, which involves training a smaller student model to replicate a larger teacher model's behavior (Bergmann, 2024). For evaluation, I plan to use metrics like accuracy, F1-score, and computational cost to assess performance. This quantitative approach will enable systematic analysis of trade-offs between accuracy and efficiency."}),"\n",(0,a.jsx)(i.p,{children:"For data collection and preprocessing, I will begin with data identification by selecting publicly available sentiment datasets like IMDb, Twitter, and Amazon product reviews. The value of these datasets is that they provide a diverse set of sentiment expressions, allowing for robust model training. The next step is data acquisition, where I will download and organize these datasets in a structured format like CSV. I will then perform exploratory data analysis to examine sentiment distribution, class counts, and potential noise in the data, which will inform our preprocessing strategy."}),"\n",(0,a.jsx)(i.p,{children:"Data preprocessing will include text cleaning, which involves removing invalid characters, HTML tags, URLs, etc. I will then tokenize the data using the appropriate tokenizer for both BERT and Llama. Next, I will implement stop-word removal to eliminate common words and focus on meaningful content. I will normalize the data by applying stemming and lemmatization to standardize the words and use label encoding to convert sentiment labels into numerical values. To address data imbalances, I will conduct a data scale analysis to identify underrepresented classes and apply techniques like oversampling or undersampling as necessary. Finally, I will split the data into training, validation, and test sets and conduct a final data verification to ensure consistency and readiness for model training."}),"\n",(0,a.jsx)(i.p,{children:"In the implementation phase, I will start by implementing BERT and Llama, then create separate optimized models using each technique: pruning, quantization, and knowledge distillation. These optimizations will be applied independently, allowing us to compare the individual impact on accuracy and computational efficiency. I will fine-tune each model on sentiment classification tasks to understand each optimization's effect on performance."}),"\n",(0,a.jsx)(i.p,{children:"For evaluation, I plan to use three metrics: accuracy, F1-score, and computational cost. Accuracy measures the proportion of correct predictions; F1-score measures the harmonic mean of precision and recall, which is critical for imbalanced datasets; and computational cost will measure memory usage and inference time. I plan to conduct multiple training trials for each model configuration to ensure reliable results. For each configuration, I will compute the mean and standard deviation of these metrics. For statistical analysis, I plan to use paired t-tests to compare the performance metrics of the base model with each optimized version. This will help determine if the observed changes in accuracy and computational efficiency are statistically significant. When comparing multiple configurations, I plan to use ANOVA to identify differences across configurations."}),"\n",(0,a.jsx)(i.p,{children:"These tests assume that data is normally distributed and has similar variances. If these conditions are not met, I will consider appropriate alternatives during project execution. This systematic approach will ensure that all conclusions are robust and data-driven."}),"\n",(0,a.jsx)(i.p,{children:"Ethical considerations are critical to ensure data privacy. I will make sure that these publicly available datasets meet data privacy requirements, are anonymized, and avoid sensitive information while aligning with privacy regulations applicable in our geographies. Bias mitigation is another important factor. Imbalanced data can result in biased model predictions favoring the majority class. I plan to address this through data preprocessing techniques like oversampling and undersampling. Transparency is key to ethical AI research. I will document all methods, evaluation metrics, and analyses clearly, allowing for reproducibility and accountability. By addressing these three ethical aspects, I am confident that we will develop a set of models that are effective and responsible."}),"\n",(0,a.jsx)(i.p,{children:"Looking at the timeline and next steps: during the first and second weeks, I plan to focus on dataset identification, acquisition, and exploratory analysis. During the third and fourth weeks, I plan to implement the base models BERT and Llama and establish the baseline performance metrics. During weeks five and six, I plan to apply optimization techniques and create a separate set of models by pruning, quantizing, and distilling, and fine-tuning each model for sentiment classification tasks. During weeks seven and eight, I plan to run multiple training trials, gather performance metrics, and conduct statistical analysis using paired t-tests and ANOVA. Finally, in weeks nine and ten, I plan to compile the results, write the research report, and prepare for the final presentation."}),"\n",(0,a.jsx)(i.p,{children:"This structured timeline will help ensure thorough exploration and evaluation of optimization techniques for transformer models."}),"\n",(0,a.jsx)(i.p,{children:"In summary, my research seeks to optimize transformer models for sentiment analysis, focusing on balancing performance with computational efficiency. By exploring optimization techniques like pruning, quantization, and knowledge distillation, the study aims to deploy cost-effective solutions without sacrificing accuracy, making sentiment analysis more accessible for real-world applications. Thank you for your attention, and thank you for listening.\u2003\nReferences"}),"\n",(0,a.jsxs)(i.p,{children:["Bergmann, D. (2024) What is knowledge distillation?, IBM. Available from: ",(0,a.jsx)(i.a,{href:"https://www.ibm.com/topics/knowledge-distillation",children:"https://www.ibm.com/topics/knowledge-distillation"})," [Accessed 06 October 2024]."]}),"\n",(0,a.jsxs)(i.p,{children:["Cambria, E. (2016) Affective Computing and Sentiment Analysis. IEEE Intelligent Systems 31(2): 102\u2013107. DOI: ",(0,a.jsx)(i.a,{href:"https://doi.org/10.1109/MIS.2016.31",children:"https://doi.org/10.1109/MIS.2016.31"})]}),"\n",(0,a.jsxs)(i.p,{children:["Clark, B. (2024) What is quantization?, IBM. Available from: ",(0,a.jsx)(i.a,{href:"https://www.ibm.com/think/topics/quantization",children:"https://www.ibm.com/think/topics/quantization"})," [Accessed 06 October 2024]."]}),"\n",(0,a.jsxs)(i.p,{children:["Devlin, J., Chang, M., Lee, K. & Toutanova, K. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Available from: ",(0,a.jsx)(i.a,{href:"https://doi.org/10.48550/arXiv.1810.04805",children:"https://doi.org/10.48550/arXiv.1810.04805"})," [Accessed 06 October 2024]."]}),"\n",(0,a.jsxs)(i.p,{children:["Liu, Z., Sun, M., Zhou, T., Huang, G. & Darrell, T. (2019) Rethinking the Value of Network Pruning. Available from: ",(0,a.jsx)(i.a,{href:"https://doi.org/10.48550/arXiv.1810.05270",children:"https://doi.org/10.48550/arXiv.1810.05270"})," [Accessed 06 October 2024]."]}),"\n",(0,a.jsxs)(i.p,{children:["Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozi\xe8re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E. & Lample, G. (2023) LLaMA: Open and Efficient Foundation Language Models. Available from: ",(0,a.jsx)(i.a,{href:"https://doi.org/10.48550/arXiv.2302.13971",children:"https://doi.org/10.48550/arXiv.2302.13971"})," [Accessed 06 October 2024]."]}),"\n",(0,a.jsxs)(i.p,{children:["Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L. & Polosukhin, I. (2017) Attention Is All You Need. Available from: ",(0,a.jsx)(i.a,{href:"https://doi.org/10.48550/arXiv.1706.03762",children:"https://doi.org/10.48550/arXiv.1706.03762"})," [Accessed 06 October 2024]."]}),"\n",(0,a.jsxs)(i.p,{children:["Zhang, L., Wang, S. & Liu, B. (2018) Deep Learning for Sentiment Analysis: A Survey. Available from: ",(0,a.jsx)(i.a,{href:"https://doi.org/10.48550/arXiv.1801.07883",children:"https://doi.org/10.48550/arXiv.1801.07883"})," [Accessed 06 October 2024]."]})]})}function h(e={}){const{wrapper:i}={...(0,n.R)(),...e.components};return i?(0,a.jsx)(i,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,i,t)=>{t.d(i,{R:()=>o,x:()=>r});var a=t(6540);const n={},s=a.createContext(n);function o(e){const i=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function r(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),a.createElement(s.Provider,{value:i},e.children)}}}]);