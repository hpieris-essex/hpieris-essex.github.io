<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-rd-docs/Unit 10: Research Proposal" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Unit 10: Research Proposal | Essex E-Portfolio</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="robots" content="noindex, nofollow"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://hpieris-essex.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://hpieris-essex.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://hpieris-essex.github.io/rd-docs/Unit 10: Research Proposal"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Unit 10: Research Proposal | Essex E-Portfolio"><meta data-rh="true" name="description" content="Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques"><meta data-rh="true" property="og:description" content="Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://hpieris-essex.github.io/rd-docs/Unit 10: Research Proposal"><link data-rh="true" rel="alternate" href="https://hpieris-essex.github.io/rd-docs/Unit 10: Research Proposal" hreflang="en"><link data-rh="true" rel="alternate" href="https://hpieris-essex.github.io/rd-docs/Unit 10: Research Proposal" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Essex E-Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Essex E-Portfolio Atom Feed"><link rel="stylesheet" href="/assets/css/styles.87564fa0.css">
<script src="/assets/js/runtime~main.1b22b622.js" defer="defer"></script>
<script src="/assets/js/main.83f450ab.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/hima_headshot_slim.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/hima_headshot_slim.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Himakara Pieris</b></a><a class="navbar__item navbar__link" href="/category/module-reflection">Machine Learning</a><a class="navbar__item navbar__link" href="/">Intelligent Agents</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/rd">Research Methods and Professional Practice</a><a href="https://github.com/hpieris-essex/hpieris-essex.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd">Research Methods &amp; Professional Practice Module Reflection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 1-3 Collaborative Discussion 1">Unit 1-3 Collaborative Discussion 1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 1-Reasoning Quiz">Unit 1-Reasoning Quiz</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 4: Case Study on Privacy">Unit 4: Case Study on Privacy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 4: Literature Review Outline">Unit 4: Literature Review Outline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 5: Inappropriate Use of Surveys">Unit 5: Inappropriate Use of Surveys</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 5: Wiki Activity — Questionnaires">Unit 5: Wiki Activity — Questionnaires</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 7-9 Collaborative Discussion 2: Case Study on Accuracy of Information">Unit 7-9 Collaborative Discussion 2: Case Study on Accuracy of Information</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 7: Inferential Statistics Workshop and Statistics Worksheet">Unit 7: Inferential Statistics Workshop and Statistics Worksheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 7: Literature Review">Unit 7: Literature Review</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/rd-docs/Unit 10: Research Proposal">Unit 10: Research Proposal</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 8-9: Statistical Worksheet and Charts">Unit 8-9: Statistical Worksheet and Charts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 12: Self Test Quiz">Unit 12: Self Test Quiz</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Unit 10: Research Proposal</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Unit 10: Research Proposal</h1></header><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimizing-transformer-models-for-sentiment-analysis-a-comparative-study-of-deep-learning-techniques">Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques<a href="#optimizing-transformer-models-for-sentiment-analysis-a-comparative-study-of-deep-learning-techniques" class="hash-link" aria-label="Direct link to Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques" title="Direct link to Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques">​</a></h3>
<iframe src="https://drive.google.com/file/d/1Zj2paOYdb9YyB5PJuUOEvuHZmJ5XL0dy/preview" width="640" height="480" allow="autoplay"></iframe>
<p>Welcome to my research proposal presentation. The title of the research is &quot;Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques.&quot; The goal of my project is to enhance sentiment analysis using transformer models like BERT and Llama.</p>
<p>Llama, similar to BERT, is designed to understand the context of words within a sentence, which is very important for accurate text analysis. However, both these models are computationally intensive. They require significant resources for training and deployment. This research adopts a conclusive research methodology to answer a specific research question and achieve well-defined objectives. Throughout this presentation, I&#x27;m going to cover the research problem, methodology, data collection, model optimization techniques, evaluation methods, and ethical considerations. Let&#x27;s get started.</p>
<p>Transformer models such as BERT and Llama have demonstrated great performance in sentiment analysis due to their ability to capture context and nuances in a document or body of text. However, these models, as I mentioned before, are expensive because they require high-end hardware like GPUs or TPUs. This results in high costs for both building and operating these models. Given this cost challenge, there is a critical need for models like these to perform sentiment analysis efficiently and cost-effectively while maintaining high accuracy. My research will explore optimization techniques that could make these transformer models more accessible and feasible for real-world applications without compromising their performance. Through my conclusive research approach, I will test various strategies to balance cost-effectiveness with accuracy.</p>
<p>The central research question in my research is: how can transformer-based models be optimized for low computational costs while maintaining accuracy? Addressing this will help make sentiment analysis more accessible by reducing the resource requirements to run powerful models.</p>
<p>We have a three-step process to answer this question.</p>
<p>The first step is to compare transformer models, specifically BERT and Llama, to evaluate their performance in sentiment analysis. This comparison will help identify each model&#x27;s strengths and limitations. Second, I will develop a set of resource-efficient transformer models by applying optimization techniques like pruning, quantization, and knowledge distillation to reduce the computational costs. Last, I will measure the cost-performance delta between the original and optimized models.</p>
<p>This involves assessing how much we can lower computational requirements without significantly compromising accuracy. Understanding this trade-off will help us recommend the most effective optimization strategies.</p>
<p>The key literature that will provide the foundation for this research includes Vaswani et al. (2017), which introduced the transformer architecture. The transformer architecture revolutionized natural language processing by using a mechanism called self-attention to capture relationships between all words in a sentence or body of text. This improves context understanding. This mechanism involves weighting each word&#x27;s importance relative to the others and is repeated multiple times using attention heads. This design enhances performance, but it is computationally intensive because as the length of the body of text grows, the memory and compute requirements can grow dramatically.</p>
<p>Building on this architecture, Devlin et al. (2019) introduced BERT, a Bidirectional Encoder Representations from Transformers. BERT uses a bidirectional approach, considering context from both sides of a word simultaneously, which increases the memory and compute requirements even more. Llama by Touvron et al. (2023), scales these models with parameters ranging from 7 billion to 70 billion. This significantly increases the computational requirements. Despite the performance, these models are costly to operate.</p>
<p>This highlights the need for optimization. Cambria (2016) and Zhang et al. (2018) discuss the additional challenges in sentiment analysis, such as imbalance and resource requirements. These challenges emphasize the importance of optimizing models to make them both accessible and efficient.</p>
<p>Let&#x27;s go over the methodology that I plan to use. As I mentioned before, I am using conclusive research. Unlike exploratory research, which is more open-ended, conclusive research is structured and aimed at providing specific, actionable results.</p>
<p>Here, I focus on optimizing transformer models to balance performance and computational cost. For data collection, I will use quantitative observation, drawing on publicly available sentiment datasets like IMDb, Twitter, and Amazon sentiment datasets to gather numerical metrics. During this process, I will conduct data quality checks to ensure the datasets are clean, balanced, and free of inconsistencies. For model selection, I plan to implement BERT and Llama, which are both publicly available through sources like Hugging Face.</p>
<p>My planned optimization techniques include pruning, which removes less significant neurons to reduce model size and computational costs (Liu et al., 2019); quantization, which involves lowering parameter precision to decrease memory usage (Clark, 2024); and knowledge distillation, which involves training a smaller student model to replicate a larger teacher model&#x27;s behavior (Bergmann, 2024). For evaluation, I plan to use metrics like accuracy, F1-score, and computational cost to assess performance. This quantitative approach will enable systematic analysis of trade-offs between accuracy and efficiency.</p>
<p>For data collection and preprocessing, I will begin with data identification by selecting publicly available sentiment datasets like IMDb, Twitter, and Amazon product reviews. The value of these datasets is that they provide a diverse set of sentiment expressions, allowing for robust model training. The next step is data acquisition, where I will download and organize these datasets in a structured format like CSV. I will then perform exploratory data analysis to examine sentiment distribution, class counts, and potential noise in the data, which will inform our preprocessing strategy.</p>
<p>Data preprocessing will include text cleaning, which involves removing invalid characters, HTML tags, URLs, etc. I will then tokenize the data using the appropriate tokenizer for both BERT and Llama. Next, I will implement stop-word removal to eliminate common words and focus on meaningful content. I will normalize the data by applying stemming and lemmatization to standardize the words and use label encoding to convert sentiment labels into numerical values. To address data imbalances, I will conduct a data scale analysis to identify underrepresented classes and apply techniques like oversampling or undersampling as necessary. Finally, I will split the data into training, validation, and test sets and conduct a final data verification to ensure consistency and readiness for model training.</p>
<p>In the implementation phase, I will start by implementing BERT and Llama, then create separate optimized models using each technique: pruning, quantization, and knowledge distillation. These optimizations will be applied independently, allowing us to compare the individual impact on accuracy and computational efficiency. I will fine-tune each model on sentiment classification tasks to understand each optimization&#x27;s effect on performance.</p>
<p>For evaluation, I plan to use three metrics: accuracy, F1-score, and computational cost. Accuracy measures the proportion of correct predictions; F1-score measures the harmonic mean of precision and recall, which is critical for imbalanced datasets; and computational cost will measure memory usage and inference time. I plan to conduct multiple training trials for each model configuration to ensure reliable results. For each configuration, I will compute the mean and standard deviation of these metrics. For statistical analysis, I plan to use paired t-tests to compare the performance metrics of the base model with each optimized version. This will help determine if the observed changes in accuracy and computational efficiency are statistically significant. When comparing multiple configurations, I plan to use ANOVA to identify differences across configurations.</p>
<p>These tests assume that data is normally distributed and has similar variances. If these conditions are not met, I will consider appropriate alternatives during project execution. This systematic approach will ensure that all conclusions are robust and data-driven.</p>
<p>Ethical considerations are critical to ensure data privacy. I will make sure that these publicly available datasets meet data privacy requirements, are anonymized, and avoid sensitive information while aligning with privacy regulations applicable in our geographies. Bias mitigation is another important factor. Imbalanced data can result in biased model predictions favoring the majority class. I plan to address this through data preprocessing techniques like oversampling and undersampling. Transparency is key to ethical AI research. I will document all methods, evaluation metrics, and analyses clearly, allowing for reproducibility and accountability. By addressing these three ethical aspects, I am confident that we will develop a set of models that are effective and responsible.</p>
<p>Looking at the timeline and next steps: during the first and second weeks, I plan to focus on dataset identification, acquisition, and exploratory analysis. During the third and fourth weeks, I plan to implement the base models BERT and Llama and establish the baseline performance metrics. During weeks five and six, I plan to apply optimization techniques and create a separate set of models by pruning, quantizing, and distilling, and fine-tuning each model for sentiment classification tasks. During weeks seven and eight, I plan to run multiple training trials, gather performance metrics, and conduct statistical analysis using paired t-tests and ANOVA. Finally, in weeks nine and ten, I plan to compile the results, write the research report, and prepare for the final presentation.</p>
<p>This structured timeline will help ensure thorough exploration and evaluation of optimization techniques for transformer models.</p>
<p>In summary, my research seeks to optimize transformer models for sentiment analysis, focusing on balancing performance with computational efficiency. By exploring optimization techniques like pruning, quantization, and knowledge distillation, the study aims to deploy cost-effective solutions without sacrificing accuracy, making sentiment analysis more accessible for real-world applications. Thank you for your attention, and thank you for listening. 
References</p>
<p>Bergmann, D. (2024) What is knowledge distillation?, IBM. Available from: <a href="https://www.ibm.com/topics/knowledge-distillation" target="_blank" rel="noopener noreferrer">https://www.ibm.com/topics/knowledge-distillation</a> [Accessed 06 October 2024].</p>
<p>Cambria, E. (2016) Affective Computing and Sentiment Analysis. IEEE Intelligent Systems 31(2): 102–107. DOI: <a href="https://doi.org/10.1109/MIS.2016.31" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/MIS.2016.31</a></p>
<p>Clark, B. (2024) What is quantization?, IBM. Available from: <a href="https://www.ibm.com/think/topics/quantization" target="_blank" rel="noopener noreferrer">https://www.ibm.com/think/topics/quantization</a> [Accessed 06 October 2024].</p>
<p>Devlin, J., Chang, M., Lee, K. &amp; Toutanova, K. (2019) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Available from: <a href="https://doi.org/10.48550/arXiv.1810.04805" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.1810.04805</a> [Accessed 06 October 2024].</p>
<p>Liu, Z., Sun, M., Zhou, T., Huang, G. &amp; Darrell, T. (2019) Rethinking the Value of Network Pruning. Available from: <a href="https://doi.org/10.48550/arXiv.1810.05270" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.1810.05270</a> [Accessed 06 October 2024].</p>
<p>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E. &amp; Lample, G. (2023) LLaMA: Open and Efficient Foundation Language Models. Available from: <a href="https://doi.org/10.48550/arXiv.2302.13971" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.2302.13971</a> [Accessed 06 October 2024].</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L. &amp; Polosukhin, I. (2017) Attention Is All You Need. Available from: <a href="https://doi.org/10.48550/arXiv.1706.03762" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.1706.03762</a> [Accessed 06 October 2024].</p>
<p>Zhang, L., Wang, S. &amp; Liu, B. (2018) Deep Learning for Sentiment Analysis: A Survey. Available from: <a href="https://doi.org/10.48550/arXiv.1801.07883" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.1801.07883</a> [Accessed 06 October 2024].</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/rd-docs/Unit 7: Literature Review"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Unit 7: Literature Review</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/rd-docs/Unit 8-9: Statistical Worksheet and Charts"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Unit 8-9: Statistical Worksheet and Charts</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#optimizing-transformer-models-for-sentiment-analysis-a-comparative-study-of-deep-learning-techniques" class="table-of-contents__link toc-highlight">Optimizing Transformer Models for Sentiment Analysis: A Comparative Study of Deep Learning Techniques</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Modules</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/">Machine Learning</a></li></ul></div><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://linkedin.com/in/hpieris" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/himakara" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Himakara Pieris.</div></div></div></footer></div>
</body>
</html>