<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-rd-docs/Unit 7: Literature Review" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Unit 7: Literature Review | Essex E-Portfolio</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="robots" content="noindex, nofollow"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://hpieris-essex.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://hpieris-essex.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://hpieris-essex.github.io/rd-docs/Unit 7: Literature Review"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Unit 7: Literature Review | Essex E-Portfolio"><meta data-rh="true" name="description" content="Implementing Deep Learning for Sentiment Analysis — A Literature Survey"><meta data-rh="true" property="og:description" content="Implementing Deep Learning for Sentiment Analysis — A Literature Survey"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://hpieris-essex.github.io/rd-docs/Unit 7: Literature Review"><link data-rh="true" rel="alternate" href="https://hpieris-essex.github.io/rd-docs/Unit 7: Literature Review" hreflang="en"><link data-rh="true" rel="alternate" href="https://hpieris-essex.github.io/rd-docs/Unit 7: Literature Review" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Essex E-Portfolio RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Essex E-Portfolio Atom Feed"><link rel="stylesheet" href="/assets/css/styles.87564fa0.css">
<script src="/assets/js/runtime~main.1b22b622.js" defer="defer"></script>
<script src="/assets/js/main.83f450ab.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/hima_headshot_slim.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/hima_headshot_slim.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Himakara Pieris</b></a><a class="navbar__item navbar__link" href="/category/module-reflection">Machine Learning</a><a class="navbar__item navbar__link" href="/">Intelligent Agents</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/rd">Research Methods and Professional Practice</a><a href="https://github.com/hpieris-essex/hpieris-essex.github.io" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd">Research Methods &amp; Professional Practice Module Reflection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 1-3 Collaborative Discussion 1">Unit 1-3 Collaborative Discussion 1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 1-Reasoning Quiz">Unit 1-Reasoning Quiz</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 4: Case Study on Privacy">Unit 4: Case Study on Privacy</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 4: Literature Review Outline">Unit 4: Literature Review Outline</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 5: Inappropriate Use of Surveys">Unit 5: Inappropriate Use of Surveys</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 5: Wiki Activity — Questionnaires">Unit 5: Wiki Activity — Questionnaires</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 7-9 Collaborative Discussion 2: Case Study on Accuracy of Information">Unit 7-9 Collaborative Discussion 2: Case Study on Accuracy of Information</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 7: Inferential Statistics Workshop and Statistics Worksheet">Unit 7: Inferential Statistics Workshop and Statistics Worksheet</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/rd-docs/Unit 7: Literature Review">Unit 7: Literature Review</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 10: Research Proposal">Unit 10: Research Proposal</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 8-9: Statistical Worksheet and Charts">Unit 8-9: Statistical Worksheet and Charts</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/rd-docs/Unit 12: Self Test Quiz">Unit 12: Self Test Quiz</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Unit 7: Literature Review</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Unit 7: Literature Review</h1></header><h3 class="anchor anchorWithStickyNavbar_LWe7" id="implementing-deep-learning-for-sentiment-analysis--a-literature-survey">Implementing Deep Learning for Sentiment Analysis — A Literature Survey<a href="#implementing-deep-learning-for-sentiment-analysis--a-literature-survey" class="hash-link" aria-label="Direct link to Implementing Deep Learning for Sentiment Analysis — A Literature Survey" title="Direct link to Implementing Deep Learning for Sentiment Analysis — A Literature Survey">​</a></h3>
<p>Introduction
This literature review explores the use of deep learning techniques in sentiment analysis. Sentiment analysis is a subfield of natural language processing (NLP). The objective of sentiment analysis is to classify and interpret subjective information from text; it’s an important topic with wide-ranging applications in areas such as social media monitoring, customer feedback analysis, and financial market predictions (Pang &amp; Lee, 2008). Traditional machine learning methods have limitations in capturing human language&#x27;s complex, context-dependent nature (Cambria, 2016). This review examines how deep learning methods — particularly recurrent neural networks (RNNs), convolutional neural networks (CNNs), and transformers have been applied to sentiment analysis.</p>
<p>Background &amp; Context
Sentiment analysis identifies and classifies opinions or emotions expressed in a text, typically as positive, negative, or neutral. Companies use sentiment analysis for social media monitoring, where they analyze social media content to gauge customer sentiment about their product or service, and in customer feedback analysis, where they analyze help desk tickets to understand possible product improvements. Outside of business settings, political organizations may use sentiment analysis to get a pulse of public opinion on various issues (Pang &amp; Lee, 2008).</p>
<p>Previous-generation sentiment analysis implementations relied on traditional machine learning techniques such as support vector machines (SVMs) and naive Bayes classifiers (Pak &amp; Paroubek, 2010). These methods require extensive feature engineering, where machine learning engineers have to work with domain experts to manually design rules and features to capture linguistic nuances (Go et al., 2009). These models struggled to capture context-dependent sentiment and were limited in their ability to handle complex sentence structures and large datasets.</p>
<p>The introduction of deep learning to sentiment analysis has provided a way to capture linguistic nuances and context without extensive feature engineering. Deep learning techniques like convolutional neural networks (CNNs), recurrent neural networks (RNNs), and, more recently, transformers have revolutionized the field by producing better-performing models. CNNs perform well for feature extraction, RNNs for capturing sequential information, and transformers like BERT and GPT show exceptional ability to understand context and relationships between words at scale. These deep learning-based models have enhanced the accuracy and efficiency of sentiment analysis, allowing systems to handle more complex language tasks and produce better results across various applications (Zhang et al., 2018).</p>
<p>Literature Search Methodology
Relevant literature was gathered using academic databases such as Google Scholar, ACL Anthology, IEEE Xplore, and ACM Digital Library. Keywords included: “deep learning”, “sentiment analysis”, “CNNs”, “RNNs”, “transformers in NLP”, and “BERT sentiment analysis”. Peer-reviewed journal articles and conference papers were prioritized to ensure the selection of credible sources.</p>
<p>The literature was organized by type of deep learning technique (e.g., CNN, RNN, transformers). Additionally, challenges such as data imbalance, model interpretability, and computational resource demands were considered, allowing for a structured comparison of different techniques and their effectiveness across various sentiment analysis tasks.</p>
<p>Deep Learning Techniques in Sentiment Analysis
Deep learning has revolutionized sentiment analysis, overcoming many limitations of traditional machine learning methods (Young et al., 2018). The key techniques include recurrent neural networks (RNNs), convolutional neural networks (CNNs), transformers like BERT, and hybrid models that combine these methods.
Recurrent Neural Networks (RNNs)
RNNs are widely used in sentiment analysis because they can process sequential data (Socher et al., 2013). They can maintain information across time steps, making them suitable for tasks that require context understanding. RNNs have an internal memory that helps capture word dependencies in sequences, which is important in longer texts where understanding the overall context is crucial.</p>
<p>However, RNNs face challenges with long-range dependencies due to the vanishing gradient problem (Trinh et al., 2018). LSTMs and GRUs were developed to address this limitation (Gref et al., 2018). LSTMs, in particular, are more effective in learning long-term dependencies. For example, studies have shown LSTMs perform better in sentiment classification tasks involving large datasets of user reviews than traditional methods like SVMs (Tai et al., 2015).
Convolutional Neural Networks (CNNs)
CNNs, typically associated with image processing, have also been applied to sentiment analysis. They apply convolutional filters to input text, extracting features such as n-grams or key phrases critical for identifying sentiment (Kim, 2014). CNNs effectively capture local dependencies and work well for short and sentiment-heavy phrases.</p>
<p>CNNs process data faster than RNNs because they don’t require sequential input processing, making them more efficient for training and inference (Kalchbrenner et al., 2014). However, they cannot capture long-range dependencies, limiting their effectiveness for more complex texts. CNNs are often combined with RNNs to leverage the strengths of both approaches and overcome this issue (Wang et al., 2016).
Transformers and BERT-like Models
Transformer models, particularly BERT and GPT, have advanced sentiment analysis by addressing key limitations of RNNs and CNNs. Transformers process the entire input sequence simultaneously using self-attention mechanisms, allowing them to capture relationships between words across the sequence (Vaswani et al., 2017). In models like BERT, this results in capturing bidirectional context — both left-to-right and right-to-left, which leads to a better understanding of words based on their surrounding context. This ability makes them highly effective for sentiment analysis, especially in nuanced and complex bodies of text.</p>
<p>BERT, trained on large corpora and fine-tuned for specific tasks, has achieved state-of-the-art results in sentiment classification (Devlin et al., 2019). Similarly, GPT models have been used to generate text embeddings that capture sentiment, with GPT-3 showing strong performance in customer feedback and social media sentiment analysis (Brown et al., 2020).</p>
<p>The scalability of transformers makes them suitable for large-scale applications, but their high computational demands present a challenge in resource-constrained environments.
Hybrid Models
Hybrid models combine different deep learning techniques to leverage their strengths. For example, CNNs and LSTMs can be integrated to extract both local features (using CNNs) and sequential information (using LSTMs). This combination has been shown to improve sentiment analysis accuracy compared to using either method alone (Wang et al., 2016).</p>
<p>Hybrid approaches that incorporate attention mechanisms further enhance sentiment analysis by allowing models to focus on the most relevant parts of the input text (Yang et al., 2016). In addition, some models integrate deep learning with traditional machine learning techniques like SVMs, where deep learning is used for feature extraction, followed by SVM classification. These hybrid models often outperform standalone approaches (Tang et al., 2015).</p>
<p>Deep learning techniques have significantly improved sentiment analysis by automating feature extraction and improving accuracy. RNNs, CNNs, and transformers each has its strengths and limitations, and hybrid models often provide more robust solutions by combining these techniques (Zhang et al., 2018).</p>
<p>Challenges and Limitations
Despite the progress deep learning has brought to sentiment analysis, several challenges still affect the accuracy and scalability of models, especially when applied to real-world data.
Data Imbalance
A common issue in sentiment analysis is dealing with imbalanced datasets. Positive sentiment dominates the data in many cases, such as product reviews or social media posts. This imbalance can lead to models biased toward the majority class, making them less accurate at predicting negative or neutral sentiment. For instance, if a dataset has 80% positive and 20% negative reviews, the model may predict positive sentiment more often, ignoring the minority class. Solutions like oversampling the minority class, under sampling the majority class, or using cost-sensitive learning have been proposed, but they add complexity and don’t fully solve the issue (Buda et al., 2018).
Context and Ambiguity
Sentiment analysis also faces challenges due to language&#x27;s context-dependent and ambiguous nature (Cambria et al., 2014). Deep learning models, while powerful, often struggle to understand shifts in sentiment based on context. A phrase like &quot;That&#x27;s just great&quot; can be interpreted as positive or negative, depending on the context. Models like BERT have improved performance by accounting for bidirectional context; However, fully grasping these subtleties remains difficult (Shangipour ataei et al., 2020). Sarcasm, irony, and idiomatic expressions also pose problems, requiring a deeper understanding of language and cultural context.
Resource Requirements
Deep learning models for sentiment analysis require significant computational resources and large datasets. Models like transformers (e.g., BERT, GPT) need powerful hardware, such as GPUs or TPUs, for training and fine-tuning (Strubell et al., 2019). This makes it difficult for smaller organizations or researchers without access to these resources. Additionally, high resource demands limit the scalability of these models in real-time applications (Brown et al., 2020). Large model sizes also create challenges for deployment on edge devices or in low-resource environments, pushing the need for more efficient models that maintain performance while reducing computational costs.</p>
<p>Comparison of Approaches
Different deep learning techniques used in sentiment analysis have their own strengths and weaknesses. RNNs, particularly LSTMs, are good at capturing sequential dependencies, which makes them effective for longer texts where context is important. However, they are computationally heavy and slow to train. CNNs, by contrast, are faster and more efficient at extracting local features, like sentiment-heavy n-grams, but they struggle with capturing long-range dependencies. Transformers, such as BERT and GPT, stand out by considering both local and global contexts with attention mechanisms, leading to better performance in most sentiment analysis tasks. However, they require a lot of computational power and memory, making them resource intensive and therefore expensive to operate.</p>
<p>Performance benchmarks show that transformers outperform traditional deep learning methods. For instance, in sentiment classification tasks, BERT-based models have achieved higher F1 scores and accuracy compared to CNNs and RNNs. A study by Devlin et al. (2019) showed BERT improving on previous state-of-the-art models by 4–8% in certain benchmarks, demonstrating its ability to handle complex sentiment analysis.</p>
<p>However, there are discrepancies in the literature. Some studies indicate that CNNs perform just as well as transformers for simpler tasks with short text, and they do so with lower computational costs (Yin et al., 2017). These findings highlight that the choice of model depends on the specific task requirements, such as dataset size, text complexity, and available resources.</p>
<p>Future Directions
Several emerging techniques are set to impact sentiment analysis. One key area is combining deep learning with explainable AI (XAI). While models like transformers deliver strong results, they often work as &quot;black boxes,&quot; making it hard to understand their decision-making process. Adding explainability to these models could improve trust and usability, especially in sectors like finance and healthcare, where understanding model decisions is particularly important (Arrieta et al., 2020). Another development is few-shot learning, which helps models learn from only a few examples. This is particularly useful when labeled data is scarce, reducing the need for large datasets (Brown et al., 2020).</p>
<p>Multilingual sentiment analysis is another area needing further development. Most sentiment analysis models are trained on English data, limiting their effectiveness globally. With the growth of social media and e-commerce, the need for models that handle multiple languages is increasing. While multilingual models like mBERT have made progress, performance still varies across languages with different structures and cultural nuances (Pires et al., 2019).</p>
<p>Ethical concerns also play a critical role. Bias in training data can lead to skewed predictions, reinforcing stereotypes or unfairly impacting certain groups (Mehrabi et al., 2021). Models trained on biased data might predict negative sentiment more often for certain demographics. Ensuring diverse, balanced datasets is essential to mitigate these risks. Transparency in how models handle sensitive data is also important to avoid misuse, especially in areas like social media monitoring.</p>
<p>Conclusion
This review examined deep learning techniques in sentiment analysis, focusing on RNNs, CNNs, and transformers. While models like BERT have improved accuracy and context handling, challenges like data imbalance, ambiguity in language, and high computational costs remain as open issues. Hybrid models that combine techniques have shown promise in addressing some of these challenges.</p>
<p>Further research is needed in key areas. First, model interpretability must improve, especially for real-world applications where understanding decisions is critical. Second, more work is required to develop multilingual models that perform consistently across diverse languages. Finally, addressing ethical concerns, such as bias in datasets and ensuring fairness in predictions, is essential for building robust and equitable sentiment analysis systems. Advancing these areas will enhance the practical use of sentiment analysis across industries.</p>
<p> 
References</p>
<p>Arrieta, A., Díaz-Rodríguez, N., Del Ser, J., Bennetot, A., Tabik, S., Barbado, A., Garcia, S., Gil-Lopez, S., Molina, D., Benjamins, R., Chatila, R. &amp; Herrera, F. (2020) Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion 58: 82–115.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I. &amp; Amodei, D. (2020) &#x27;Language models are few-shot learners&#x27;, Advances in Neural Information Processing Systems. Virtual Conference, 6–12 December. New York: NeurIPS. 1877–1901.</p>
<p>Buda, M., Maki, A. &amp; Mazurowski, M.A. (2018) A systematic study of the class imbalance problem in convolutional neural networks. Neural Networks 106: 249–259.</p>
<p>Cambria, E. (2016) Affective computing and sentiment analysis. IEEE Intelligent Systems 31(2): 102-107.</p>
<p>Cambria, E., Olsher, D. &amp; Rajagopal, D. (2014) &#x27;SenticNet 3: A common and common-sense knowledge base for cognition-driven sentiment analysis&#x27;, The 28th AAAI Conference on Artificial Intelligence. Quebec City, Canada, 27–31 July. Menlo Park: AAAI Press. 1515–1521.</p>
<p>Devlin, J., Chang, M.W., Lee, K. &amp; Toutanova, K. (2019) &#x27;BERT: Pre-training of deep bidirectional transformers for language understanding&#x27;, The 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Minneapolis, Minnesota, 2–7 June. Stroudsburg: Association for Computational Linguistics. 4171–4186.</p>
<p>Go, A., Bhayani, R. &amp; Huang, L. (2009) Twitter sentiment classification using distant supervision. Palo Alto: Stanford University.</p>
<p>Greff, K., Srivastava, R.K., Koutník, J., Steunebrink, B.R. &amp; Schmidhuber, J. (2017) LSTM: A Search Space Odyssey. IEEE Transactions on Neural Networks and Learning Systems 28(10): 2222–2232.</p>
<p>Kalchbrenner, N., Grefenstette, E. &amp; Blunsom, P. (2014) &#x27;A Convolutional Neural Network for Modelling Sentences&#x27;, The 52nd Annual Meeting of the Association for Computational Linguistics. Baltimore, Maryland, 22–27 June. Stroudsburg: Association for Computational Linguistics. 655–665.</p>
<p>Kim, Y. (2014) &#x27;Convolutional Neural Networks for Sentence Classification&#x27;, The 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar, 25–29 October. Stroudsburg: Association for Computational Linguistics. 1746–1751.</p>
<p>Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K. &amp; Galstyan, A. (2021) A Survey on Bias and Fairness in Machine Learning. ACM Computing Surveys 54(6): 115. DOI: <a href="https://doi.org/10.1145/3457607" target="_blank" rel="noopener noreferrer">https://doi.org/10.1145/3457607</a>.</p>
<p>Pak, A. &amp; Paroubek, P. (2010) &#x27;Twitter as a Corpus for Sentiment Analysis and Opinion Mining&#x27;, The Seventh International Conference on Language Resources and Evaluation (LREC&#x27;10). Valletta, Malta, 17–23 May. Paris: European Language Resources Association (ELRA).</p>
<p>Pang, B. &amp; Lee, L. (2008) Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval 2(1–2): 1–135. DOI: <a href="https://doi.org/10.1561/1500000011" target="_blank" rel="noopener noreferrer">https://doi.org/10.1561/1500000011</a></p>
<p>Pires, T., Schlinger, E. &amp; Garrette, D. (2019) How multilingual is Multilingual BERT?. Available from: <a href="https://doi.org/10.48550/arXiv.1906.01502" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.1906.01502</a> [Accessed 16 September 2024].</p>
<p>Shangipour Ataei, T., Javdan, S. &amp; Minaei-Bidgoli, B. (2020) &#x27;Applying Transformers and Aspect-based Sentiment Analysis approaches on Sarcasm Detection&#x27;, The Second Workshop on Figurative Language Processing. Online, 9 July. Stroudsburg: Association for Computational Linguistics. 67–71.</p>
<p>Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.D., Ng, A. &amp; Potts, C. (2013) &#x27;Recursive deep models for semantic compositionality over a sentiment treebank&#x27;, Conference on Empirical Methods in Natural Language Processing (EMNLP). Seattle, 18–21 October. Stroudsburg: Association for Computational Linguistics. 1631–1642.</p>
<p>Strubell, E., Ganesh, A. &amp; McCallum, A. (2019) &#x27;Energy and Policy Considerations for Deep Learning in NLP&#x27;, The 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy, 28 July–2 August. Stroudsburg: Association for Computational Linguistics. 3645–3650.</p>
<p>Tai, K.S., Socher, R. &amp; Manning, C.D. (2015) &#x27;Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks&#x27;, The 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Beijing, China, 26–31 July. Stroudsburg: Association for Computational Linguistics. 1556–1566.</p>
<p>Tang, D., Qin, B. &amp; Liu, T. (2015) &#x27;Document Modeling with Gated Recurrent Neural Network for Sentiment Classification&#x27;, The 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal, 17–21 September. Stroudsburg: Association for Computational Linguistics. 1422–1432.</p>
<p>Trinh, T., Dai, A., Luong, T. &amp; Le, Q. (2018) &#x27;Learning Longer-term Dependencies in RNNs with Auxiliary Losses&#x27;, The 35th International Conference on Machine Learning. Stockholm, Sweden, 10–15 July. London: PMLR. 4965–4974.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L. &amp; Polosukhin, I. (2017) &#x27;Attention is all you need&#x27;, Advances in Neural Information Processing Systems. Long Beach, California, 4–9 December. New York: NeurIPS.</p>
<p>Wang, X., Jiang, W. &amp; Luo, Z. (2016) &#x27;Combination of Convolutional and Recurrent Neural Network for Sentiment Analysis of Short Texts&#x27;, The 26th International Conference on Computational Linguistics (COLING 2016). Osaka, Japan, 11–16 December. Osaka: The COLING 2016 Organizing Committee. 2428–2437.</p>
<p>Yang, Z., Yang, D., Dyer, C., He, X., Smola, A. &amp; Hovy, E. (2016) Hierarchical attention networks for document classification. Available from: <a href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" target="_blank" rel="noopener noreferrer">https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf</a> [Accessed 16 September 2024].</p>
<p>Yin, W., Kann, K., Yu, M. &amp; Schütze, H. (2017) Comparative Study of CNN and RNN for Natural Language Processing. Available from: <a href="https://doi.org/10.48550/arXiv.1702.01923" target="_blank" rel="noopener noreferrer">https://doi.org/10.48550/arXiv.1702.01923</a> [Accessed 16 September 2024].</p>
<p>Young, T., Hazarika, D., Poria, S. &amp; Cambria, E. (2018) Recent Trends in Deep Learning Based Natural Language Processing [Review Article]. IEEE Computational Intelligence Magazine 13(3): 55–75.</p>
<p>Zhang, L., Wang, S. &amp; Liu, B. (2018) Deep learning for sentiment analysis: A survey. WIREs Data Mining and Knowledge Discovery 8(4): e1253.</p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/rd-docs/Unit 7: Inferential Statistics Workshop and Statistics Worksheet"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Unit 7: Inferential Statistics Workshop and Statistics Worksheet</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/rd-docs/Unit 10: Research Proposal"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Unit 10: Research Proposal</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#implementing-deep-learning-for-sentiment-analysis--a-literature-survey" class="table-of-contents__link toc-highlight">Implementing Deep Learning for Sentiment Analysis — A Literature Survey</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Modules</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/">Machine Learning</a></li></ul></div><div class="col footer__col"><div class="footer__title">Social</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://linkedin.com/in/hpieris" target="_blank" rel="noopener noreferrer" class="footer__link-item">LinkedIn<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/himakara" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 Himakara Pieris.</div></div></div></footer></div>
</body>
</html>